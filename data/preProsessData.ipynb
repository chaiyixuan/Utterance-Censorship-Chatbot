{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'isis.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-582cac955049>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_words_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'isis.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#all_words_path='isisFanboy.txt'\n",
    "#all_words_path='abouisist.txt'\n",
    "all_words_path='isis.txt'\n",
    "line=[]\n",
    "lines=[]\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "captions = np.asarray(corpus, dtype=np.object)\n",
    "captions = map(lambda x: x.replace('.', ''), captions)\n",
    "captions = map(lambda x: x.replace(',', ''), captions)\n",
    "captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "captions = map(lambda x: x.replace('?', ''), captions)\n",
    "captions = map(lambda x: x.replace('!', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "captions = map(lambda x: x.replace('/', ''), captions)\n",
    "captions = map(lambda x: x.replace('RT', ''), captions)\n",
    "captions = map(lambda x: x.replace('ENGLISH TRANSLATION', ''), captions)\n",
    "\n",
    "captions=list(captions)\n",
    "\n",
    "for sent in captions:\n",
    "    for w in sent.lower().split(' '):\n",
    "        if \"http\" in w or \"@\" in w or \"#\" in w:\n",
    "            continue\n",
    "        w=re.findall(\"[a-zA-Z'-]+\", w)\n",
    "        if w!=[]:\n",
    "            line.append(w)\n",
    "    lines.append(line)\n",
    "    line=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ISISProed.txt', 'w',encoding=\"utf-8\") as f:\n",
    "   \n",
    "    for line in lines:\n",
    "        lineStr=\"\"\n",
    "        for w in line:\n",
    "            if lineStr==\"\":\n",
    "                lineStr=w[0]\n",
    "            else:\n",
    "                lineStr=lineStr+' '+w[0]\n",
    "        if(lineStr!=\"\"):\n",
    "            lineStr+='\\n'\n",
    "            f.write(lineStr)\n",
    "        #print(lineStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json 解析到txt\n",
    "import json\n",
    "\n",
    "#all_words_path='isisFanboy.txt'\n",
    "all_words_path='nazi.json'\n",
    "\n",
    "data = open(all_words_path, 'r',encoding=\"utf-8\").read()\n",
    "jobjLisat=json.loads(data)\n",
    "\n",
    "with open('NaziText.txt', 'w',encoding=\"utf-8\") as f:\n",
    "   \n",
    "    for sens in jobjLisat:\n",
    "        f.write(sens[\"full_text\"]+'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_path='NaziText.txt'\n",
    "line=[]\n",
    "lines=[]\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "captions = np.asarray(corpus, dtype=np.object)\n",
    "captions = map(lambda x: x.replace('.', ''), captions)\n",
    "captions = map(lambda x: x.replace(',', ''), captions)\n",
    "captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "captions = map(lambda x: x.replace('?', ''), captions)\n",
    "captions = map(lambda x: x.replace('!', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "captions = map(lambda x: x.replace('/', ''), captions)\n",
    "captions = map(lambda x: x.replace('RT', ''), captions)\n",
    "captions = map(lambda x: x.replace('ENGLISH TRANSLATION', ''), captions)\n",
    "\n",
    "captions=list(captions)\n",
    "\n",
    "for sent in captions:\n",
    "    for w in sent.lower().split(' '):\n",
    "        if \"http\" in w or \"@\" in w or \"#\" in w:\n",
    "            continue\n",
    "        w=re.findall(\"[a-zA-Z'-]+\", w)\n",
    "        if w!=[]:\n",
    "            line.append(w)\n",
    "    lines.append(line)\n",
    "    line=[]\n",
    "with open('NaziTextProed.txt', 'w',encoding=\"utf-8\") as f:\n",
    "   \n",
    "    for line in lines:\n",
    "        lineStr=\"\"\n",
    "        for w in line:\n",
    "            if lineStr==\"\":\n",
    "                lineStr=w[0]\n",
    "            else:\n",
    "                lineStr=lineStr+' '+w[0]\n",
    "        if(lineStr!=\"\"):\n",
    "            lineStr+='\\n'\n",
    "            f.write(lineStr)\n",
    "        #print(lineStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterWordsList\n",
    "filterWordsList = open(\"badWordsList.txt\", 'r',encoding=\"utf-8\").read().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#纳粹语料筛选\n",
    "corpus = open(\"NaziTextProed.txt\", 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "with open('NaziTextSelected.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for line in corpus:\n",
    "        result=list(map(lambda x:x in line  ,filterWordsList))\n",
    "        result=np.sum(result)\n",
    "        if(result >0):\n",
    "            #print(line)\n",
    "            f.write(line+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISIS语料筛选\n",
    "corpus = open(\"ISISProed.txt\", 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "with open('ISISProedSelect.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for line in corpus:\n",
    "        result=list(map(lambda x:x in line  ,filterWordsList))\n",
    "        result=np.sum(result)\n",
    "        if(result >0):\n",
    "            #print(line)\n",
    "            f.write(line+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bd Callgirl\n",
      "bd Callgirl\n",
      "bd Callgirl\n"
     ]
    }
   ],
   "source": [
    "badlist=['a','boy','cry']\n",
    "corpus=[\"i have a pen\",\"i dont kown\",\"boy sit down\"]\n",
    "for line in corpus:\n",
    "    result=list(map(lambda x:x in line  ,filterWordsList))\n",
    "    print(\"bd\",filterWordsList[53])\n",
    "    #print(result)\n",
    "    result=np.sum(result)\n",
    "    if(result >0):\n",
    "        print(line)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffjewishn', 'bolshevism', 'sex', 'whore', 'homosexual', 'hell', 'isis', 'Hitler', 'Fascist', 'Nazi', 'Alcoholic', 'Amateur', 'Analphabet', 'Anarchist', 'Ape', 'Arse', 'Arselicker', 'Ass', 'Ass master', 'Ass-kisser', 'Ass-nugget', 'Ass-wipe', 'Asshole', 'Baby', 'Backwoodsman', 'Balls', 'Bandit', 'Barbar', 'Bastard', 'Bastard', 'Beavis', 'Beginner', 'Biest', 'Bitch', 'Blubber gut', 'Bogeyman', 'Booby', 'Boozer', 'Bozo', 'Brain-fart', 'Brainless', 'Brainy', 'Brontosaurus', 'Brownie', 'Bugger', 'Bugger, silly', 'Bulloks', 'Bum', 'Bum-fucker', 'Butt', 'Buttfucker', 'Butthead', 'C', '', 'Callboy', 'Callgirl', 'Camel', 'Cannibal', 'Cave man', 'Chaavanist', 'Chaot', 'Chauvi', 'Cheater', 'Chicken', 'Children fucker', 'Clit', 'Clown', 'Cock', 'Cock master', 'Cock up', 'Cockboy', 'Cockfucker', 'Cockroach', 'Coky', 'Con merchant', 'Con-man', 'Country bumpkin', 'Cow', 'Creep', 'Creep', 'Cretin', 'Criminal', 'Cunt', 'Cunt sucker', 'Daywalker', 'Deathlord', 'Derr brain', 'Desperado', 'Devil', 'Dickhead', 'Dinosaur', 'Disguesting packet', 'Diz brain', 'Do-Do', 'Dog', 'Dog, dirty', 'Dogshit', 'Donkey', 'Drakula', 'Dreamer', 'Drinker', 'Drunkard', 'Dufus', 'Dulles', 'Dumbo', 'Dummy', 'Dumpy', 'Egoist', 'Eunuch', 'Exhibitionist', 'Fake', 'Fanny', 'Farmer', 'Fart', 'Fart, shitty', 'Fatso', 'Fellow', 'Fibber', 'Fish', 'Fixer', 'Flake', 'Flash Harry', 'Freak', 'Frog', 'Fuck', 'Fuck face', 'Fuck head', 'Fuck noggin', 'Fucker', 'Gangster', 'Ghost', 'Goose', 'Gorilla', 'Grouch', 'Grumpy', 'Head, fat', 'Hell dog', 'Hillbilly', 'Hippie', 'Homo', 'Homosexual', 'Hooligan', 'Horse fucker', 'Idiot', 'Ignoramus', 'Jack-ass', 'Jerk', 'Joker', 'Junkey', 'Killer', 'Lard face', 'Latchkey child', 'Learner', 'Liar', 'Looser', 'Lucky', 'Lumpy', 'Luzifer', 'Macho', 'Macker', 'Man, old', 'Minx', 'Missing link', 'Monkey', 'Monster', 'Motherfucker', 'Mucky pub', 'Mutant', 'Neanderthal', 'Nerfhearder', 'Nobody', 'Nurd', 'Nuts, numb', 'Oddball', 'Oger', 'Oil dick', 'Old fart', 'Orang-Uthan', 'Original', 'Outlaw', 'Pack', 'Pain in the ass', 'Pavian', 'Pencil dick', 'Pervert', 'Pig', 'Piggy-wiggy', 'Pirate', 'Pornofreak', 'Prick', 'Prolet', 'Queer', 'Querulant', 'Rat', 'Rat-fink', 'Reject', 'Retard', 'Riff-Raff', 'Ripper', 'Roboter', 'Rowdy', 'Rufian', 'Sack', 'Sadist', 'Saprophyt', 'Satan', 'Scarab', 'Schfincter', 'Shark', 'Shit eater', 'Shithead', 'Simulant', 'Skunk', 'Skuz bag', 'Slave', 'Sleeze', 'Sleeze bag', 'Slimer', 'Slimy bastard', 'Small pricked', 'Snail', 'Snake', 'Snob', 'Snot', 'Son of a bitch ', 'Square', 'Stinker', 'Stripper', 'Stunk', 'Swindler', 'Swine', 'Teletubby', 'Thief', 'Toilett cleaner', 'Tussi', 'Typ', 'Unlike', 'Vampir', 'Vandale', 'Varmit', 'Wallflower', 'Wanker', 'Wanker, bloody', 'Weeze Bag', 'Whore', 'Wierdo', 'Wino', 'Witch', 'Womanizer', 'Woody allen', 'Worm', 'Xena', 'Xenophebe', 'Xenophobe', 'XXX Watcher', 'Yak', 'Yeti', 'Zit fac']\n"
     ]
    }
   ],
   "source": [
    "print(filterWordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dec.txt 正样品提取\n",
    "# filterWordsList\n",
    "filterWordsList = open(\"filterWordsList.txt\", 'r',encoding=\"utf-8\").read().split('\\n')\n",
    "corpus = open(\"train.dec.txt\", 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "with open('goodlines.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for line in corpus:\n",
    "        result=list(map(lambda x:x in line  ,filterWordsList))\n",
    "        result=np.sum(result)\n",
    "        if(result ==0):\n",
    "            #print(line)\n",
    "            f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_path='goodlines.txt'\n",
    "line=[]\n",
    "lines=[]\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "captions = np.asarray(corpus, dtype=np.object)\n",
    "captions = map(lambda x: x.replace('.', ''), captions)\n",
    "captions = map(lambda x: x.replace(',', ''), captions)\n",
    "captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "captions = map(lambda x: x.replace('?', ''), captions)\n",
    "captions = map(lambda x: x.replace('!', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "captions = map(lambda x: x.replace('/', ''), captions)\n",
    "\n",
    "\n",
    "captions=list(captions)\n",
    "\n",
    "for sent in captions:\n",
    "    for w in sent.lower().split(' '):\n",
    "        if \"http\" in w or \"@\" in w or \"#\" in w:\n",
    "            continue\n",
    "        w=re.findall(\"[a-zA-Z'-]+\", w)\n",
    "        if w!=[]:\n",
    "            line.append(w)\n",
    "    lines.append(line)\n",
    "    line=[]\n",
    "with open('goodlines.txt', 'w',encoding=\"utf-8\") as f:\n",
    "   \n",
    "    for line in lines:\n",
    "        lineStr=\"\"\n",
    "        for w in line:\n",
    "            if lineStr==\"\":\n",
    "                lineStr=w[0]\n",
    "            else:\n",
    "                lineStr=lineStr+' '+w[0]\n",
    "        if(lineStr!=\"\"):\n",
    "            lineStr+='\\n'\n",
    "            f.write(lineStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## twitter_en 数据预处理  \n",
    "for charbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#all_words_path='isisFanboy.txt'\n",
    "#all_words_path='abouisist.txt'\n",
    "all_words_path='chat.txt'\n",
    "line=[]\n",
    "twittes_lines=[]\n",
    "response_lines=[]\n",
    "\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "captions = np.asarray(corpus, dtype=np.object)\n",
    "captions = map(lambda x: x.replace('.', ''), captions)\n",
    "captions = map(lambda x: x.replace(',', ''), captions)\n",
    "captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "captions = map(lambda x: x.replace('?', ''), captions)\n",
    "captions = map(lambda x: x.replace('!', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "captions = map(lambda x: x.replace('/', ''), captions)\n",
    "captions = map(lambda x: x.replace('RT', ''), captions)\n",
    "captions = map(lambda x: x.replace('ENGLISH TRANSLATION', ''), captions)\n",
    "\n",
    "captions=list(captions)\n",
    "## odd lines are tweet and even lines are corresponding responded tweets. \n",
    "lineNum=1;\n",
    "for sent in captions:\n",
    "    for w in sent.lower().split(' '):\n",
    "        if \"http\" in w or \"@\" in w or \"#\" in w:\n",
    "            continue\n",
    "        w=re.findall(\"[a-zA-Z'-]+\", w)\n",
    "        if w!=[]:\n",
    "            line.append(w)\n",
    "    ## odd lines are tweet and even lines are corresponding responded tweets. \n",
    "    if(lineNum%2==0):    \n",
    "        response_lines.append(line)\n",
    "        line=[]\n",
    "    else:\n",
    "        twittes_lines.append(line)\n",
    "        line=[]\n",
    "    lineNum+=1\n",
    "        \n",
    "with open('ask_en.txt', 'w',encoding=\"utf-8\") as fa:\n",
    "    with open('res_en.txt', 'w',encoding=\"utf-8\") as fr:\n",
    "        for line in twittes_lines:\n",
    "            lineStr=\"\"\n",
    "            for w in line:\n",
    "                if lineStr==\"\":\n",
    "                    lineStr=w[0]\n",
    "                else:\n",
    "                    lineStr=lineStr+' '+w[0]\n",
    "            if(lineStr!=\"\"):\n",
    "                lineStr+='\\n'\n",
    "                fa.write(lineStr)\n",
    "       \n",
    "        for line in response_lines:\n",
    "            lineStr=\"\"\n",
    "            for w in line:\n",
    "                if lineStr==\"\":\n",
    "                    lineStr=w[0]\n",
    "                else:\n",
    "                    lineStr=lineStr+' '+w[0]\n",
    "            if(lineStr!=\"\"):\n",
    "                lineStr+='\\n'\n",
    "                fr.write(lineStr)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCgoodLine数据预处理 \n",
    "去掉一些符号，并选取一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#all_words_path='isisFanboy.txt'\n",
    "#all_words_path='abouisist.txt'\n",
    "all_words_path='TCgoodLineAll.txt'\n",
    "line=[]\n",
    "all_lines=[]\n",
    "\n",
    "\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "captions = np.asarray(corpus, dtype=np.object)\n",
    "captions = map(lambda x: x.replace('.', ''), captions)\n",
    "captions = map(lambda x: x.replace(',', ''), captions)\n",
    "captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "captions = map(lambda x: x.replace('?', ''), captions)\n",
    "captions = map(lambda x: x.replace('!', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "captions = map(lambda x: x.replace('/', ''), captions)\n",
    "captions = map(lambda x: x.replace('RT', ''), captions)\n",
    "captions = map(lambda x: x.replace('ENGLISH TRANSLATION', ''), captions)\n",
    "\n",
    "captions=list(captions)\n",
    "## odd lines are tweet and even lines are corresponding responded tweets. \n",
    "\n",
    "for sent in captions:# every line\n",
    "    for w in sent.lower().split(' '):\n",
    "        if \"http\" in w or \"@\" in w or \"#\" in w:\n",
    "            continue\n",
    "        w=re.findall(\"[a-zA-Z'-]+\", w)\n",
    "        if w!=[]:\n",
    "            line.append(w)\n",
    "    all_lines.append(line)\n",
    "    line=[]\n",
    "\n",
    "\n",
    "#fileName=\"ISISgoodLine\"\n",
    "#lineLimit=17576\n",
    "fileName=\"NazigoodLine\"\n",
    "lineLimit=113265\n",
    "\n",
    "lineNum=0\n",
    "with open(fileName, 'w',encoding=\"utf-8\") as f:\n",
    "    for line in all_lines:# every line\n",
    "        lineNum=lineNum+1\n",
    "        if(lineNum<lineLimit):\n",
    "            lineStr=\"\"\n",
    "            for w in line:#every words\n",
    "                #print(line)\n",
    "                if lineStr==\"\":\n",
    "                    lineStr=w[0]\n",
    "                else:\n",
    "                    lineStr=lineStr+' '+w[0]\n",
    "            if(lineStr!=\"\"):\n",
    "                lineStr+='\\n'\n",
    "                f.write(lineStr)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "                \n",
    "       \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i l c\n"
     ]
    }
   ],
   "source": [
    "line=['i','love','china']\n",
    "lineStr=\"\"\n",
    "for w in line:#every words\n",
    "    #print(w)\n",
    "    if lineStr==\"\":\n",
    "        lineStr=w[0]\n",
    "    else:\n",
    "        lineStr=lineStr+' '+w[0]\n",
    "print(lineStr)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture数据预处理\n",
    "从每个数据集随机选5K条\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "all_words_path='TCbadLine.txt'\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "#print(corpus[0])\n",
    "Lines1=random.sample(corpus,5000)\n",
    "\n",
    "all_words_path='ISISbadLine.txt'\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "#print(corpus[0])\n",
    "Lines2=random.sample(corpus,5000)\n",
    "\n",
    "all_words_path='NazibadLine.txt'\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "#print(corpus[0])\n",
    "Lines3=random.sample(corpus,5000)\n",
    "\n",
    "Lines=Lines1+Lines2+Lines3\n",
    "print(len(Lines))\n",
    "with open('MixturebadLine.txt', 'w',encoding=\"utf-8\") as f:\n",
    "   \n",
    "    for line in Lines:\n",
    "        f.write(line+'\\n')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "all_words_path='TCgoodLineAll.txt'\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')[:-1]\n",
    "#print(corpus[0])\n",
    "Lines=random.sample(corpus,15000)\n",
    "\n",
    "print(len(Lines))\n",
    "with open('MixturegoodLine.txt', 'w',encoding=\"utf-8\") as f:\n",
    "   \n",
    "    for line in Lines:\n",
    "        f.write(line+'\\n')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缩短badLine的 长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['andante'], ['get'], ['a'], ['life']], [['king'], ['of'], ['hearts'], ['is'], ['a'], ['faggot'], ['that'], ['is'], ['all']], [['the'], ['only'], ['vandals'], ['are'], ['pathetic'], ['wiki'], ['administrators'], ['as'], ['encyclopediadramatica', 'com'], ['says'], ['right']], [['fat'], ['niggers'], [\"shouldn't\"], ['it'], ['be'], ['mentioned'], ['that'], ['upn'], ['was'], ['a'], ['network'], ['for'], ['fat'], ['niggers']], [['kind'], ['of'], ['a'], ['stupid'], ['response'], ['given'], ['the'], ['name'], ['of'], ['the'], ['article']], [['rubbish'], ['i'], ['do'], ['not'], ['support'], ['your'], ['crappy'], ['encyclopedia'], ['and'], [\"that's\"], ['that']], [['your'], ['a'], ['fucking'], ['wanker'], ['lol']], [['note', 'if'], ['ya'], ['aint'], ['dutch'], ['ya'], ['aint'], ['much']], [['pussy'], ['bitch'], ['delete'], ['things']], [['george'], ['why'], ['did'], ['you'], ['come'], ['here'], ['to'], ['tell'], ['such'], ['lies']]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "all_words_path='MixturebadLine.txt'\n",
    "corpus = open(all_words_path, 'r',encoding=\"utf-8\").read().split('\\n')\n",
    "captions = np.asarray(corpus, dtype=np.object)\n",
    "captions = map(lambda x: x.replace('\"', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\n', ''), captions)\n",
    "captions = map(lambda x: x.replace('!', ''), captions)\n",
    "captions = map(lambda x: x.replace('\\\\', ''), captions)\n",
    "captions = map(lambda x: x.replace('/', ''), captions)\n",
    "\n",
    "captions=list(captions)\n",
    "line=[]\n",
    "lines=[]\n",
    "max_seq=20\n",
    "max_char=150\n",
    "min_seq=2\n",
    "for sent in corpus:\n",
    "    #sent=sent.split('.')[0]#只取句号前面的\n",
    "    \n",
    "    wordList=sent.lower().split(' ')\n",
    "    if(len(wordList)>max_seq or len(sent)>max_char or len(wordList)<min_seq):# 只小于20个词的和小雨\n",
    "        continue\n",
    "    for w in wordList:\n",
    "        if \"http\" in w or \"@\" in w or \"#\" in w:\n",
    "            continue\n",
    "        w=re.findall(\"[a-zA-Z'-]+\", w)\n",
    "        if w!=[]:\n",
    "            #w=w[0]\n",
    "            line.append(w)\n",
    "    lines.append(line)\n",
    "    line=[]\n",
    "print(lines[:10])\n",
    "\n",
    "with open('MixturebadLineShort.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for line in lines:\n",
    "        lineStr=\"\"\n",
    "        for w in line:\n",
    "            if lineStr==\"\":\n",
    "                lineStr=w[0]\n",
    "            else:\n",
    "                lineStr=lineStr+' '+w[0]\n",
    "        if(lineStr!=\"\"):\n",
    "            lineStr+='\\n'\n",
    "            f.write(lineStr)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
