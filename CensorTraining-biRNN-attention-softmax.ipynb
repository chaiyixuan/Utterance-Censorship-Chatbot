{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.util import compat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def task_specific_attention(inputs, output_size,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            activation_fn=tf.tanh, scope=None):\n",
    "\n",
    "    assert len(inputs.get_shape()) == 3 and inputs.get_shape()[-1].value is not None\n",
    "\n",
    "    with tf.variable_scope(scope or 'attention') as scope:\n",
    "        attention_context_vector = tf.get_variable(name='attention_context_vector',\n",
    "                                                   shape=[output_size],\n",
    "                                                   initializer=initializer,\n",
    "                                                   dtype=tf.float32)\n",
    "        input_projection = tf.contrib.layers.fully_connected(inputs, output_size,\n",
    "                                                  activation_fn=activation_fn)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2))\n",
    "        attention_weights = tf.expand_dims(attention_weights, -1)#在最后一列加一个1，扩充一列\n",
    "        weighted_projection = tf.multiply(attention_weights, inputs)\n",
    "        outputs = tf.reduce_sum(weighted_projection, axis=1)\n",
    "        return outputs,attention_weights\n",
    "\n",
    "\n",
    "    \n",
    "class Network(object):\n",
    "    def __init__(self,\n",
    "             \n",
    "                 n_hidden = 64,\n",
    "                 dropout_keep_prob=0.8\n",
    "                 ):\n",
    "   \n",
    "        #n_hidden = 1024 # hidden layer num of features\n",
    "        self.x = tf.placeholder(\"float\", [None, None, alphabet_size], name=\"inputs\")\n",
    "        self.y = tf.placeholder(tf.int64, [None, ], name=\"labels\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        #n_hidden = 64 # hidden layer num of features\n",
    "        self.network = tl.layers.InputLayer(self.x, name='input_layer')\n",
    "        self.network = tl.layers.BiDynamicRNNLayer(self.network,\n",
    "            cell_fn         = tf.contrib.rnn.BasicLSTMCell,\n",
    "            n_hidden        = n_hidden,\n",
    "            dropout         = dropout_keep_prob,\n",
    "            sequence_length = tl.layers.retrieve_seq_length_op(self.x),\n",
    "            return_seq_2d   = False,\n",
    "            return_last     = False,\n",
    "            n_layer         = 3,\n",
    "            name            = 'dynamic_rnn1')\n",
    "        character_encoder_output = self.network.outputs\n",
    "\n",
    "            \n",
    "        with tf.variable_scope('attention') as scope:\n",
    "            character_level_output,attention_weights = task_specific_attention(\n",
    "                character_encoder_output, 250, scope=scope)\n",
    "\n",
    "        with tf.variable_scope('classifier'):\n",
    "            self.logits = tf.contrib.layers.fully_connected(\n",
    "                character_level_output, 2, activation_fn=None)\n",
    "\n",
    "                                     \n",
    "        #=========\n",
    "        self.attention_weights=attention_weights\n",
    "       \n",
    "   \n",
    "        #########\n",
    "        self.network.outputs_op = tf.argmax(self.logits, axis=-1)\n",
    "        \n",
    "\n",
    "        self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y,logits=self.logits)\n",
    "        self.loss = tf.reduce_mean(self.loss)\n",
    "            \n",
    "  \n",
    "\n",
    "        #self.loss      = tl.cost.cross_entropy(self.logits, self.y, 'xentropy')\n",
    "\n",
    "\n",
    "        self.accuracy  = tf.reduce_mean(tf.cast(tf.nn.in_top_k(self.logits, self.y, 1), tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:83: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "one_hot_list=[]\n",
    "alpha2id_dict={}\n",
    "alphabet=\" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "alphabet_size = len(alphabet)\n",
    "one_hot_list = np.eye(alphabet_size, dtype='int64')\n",
    "for i, c in enumerate(alphabet):\n",
    "    alpha2id_dict[c] = i + 1\n",
    "#print(one_hot_list)\n",
    "#print(alpha2id_dict)\n",
    "sqe_limit=256## max length of sequence\n",
    "#dataset_name=\"TC\"  ##toxic comment\n",
    "#dataset_name=\"ISIS\" ## isis fans\n",
    "#dataset_name=\"Nazi\"# nazi \n",
    "#dataset_name=\"Mixture\" #Mixture\n",
    "#dataset_name=\"Word\" #wordList\n",
    "#dataset_name=\"Chat\"\n",
    "dataset_name=\"test\"\n",
    "def load_dataset(test_size=0.2):\n",
    "    \"\"\"加载样本并取test_size的比例做测试集\n",
    "    Args:\n",
    "        files: 样本文件目录集合\n",
    "            样本文件是包含了样本特征向量与标签的npy文件\n",
    "        test_size: float\n",
    "            0.0到1.0之间，代表数据集中有多少比例抽做测试集\n",
    "    Returns:\n",
    "        X_train, y_train: 训练集特征列表和标签列表\n",
    "        X_test, y_test: 测试集特征列表和标签列表0\n",
    "    \"\"\"\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    #=====\n",
    "    for label in [\"good\", \"bad\"]:\n",
    "        samples = []\n",
    "        show=[]\n",
    "        #inp = \"data/\" + label + \"Lines.txt\"\n",
    "        inp = \"data/\"+dataset_name + label + \"Line.txt\" \n",
    "        f = open(inp,encoding=\"utf-8\")\n",
    "        for line in f:\n",
    "            #words = list(line.strip())\n",
    "            line=line.lower()\n",
    "            #print(line)\n",
    "            words = list(line)\n",
    "            #print(words)\n",
    "            #if(label==\"bad\"):\n",
    "            #    print(words)\n",
    "            text_sequence = []# every line\n",
    "           \n",
    "            #for word in words:#every character\n",
    "            for i,word in enumerate(words):#every character\n",
    "                try:\n",
    "                    if(i<sqe_limit):\n",
    "                        #print(word)\n",
    "                        #print(\"word\",word,one_hot_list[alpha2id_dict[word]])\n",
    "                        text_sequence.append(one_hot_list[alpha2id_dict[word]])\n",
    "                        #print(\"ok\")\n",
    "                    else:\n",
    "                        #print(\"cut\")\n",
    "                        break\n",
    "                except:\n",
    "                    #print(\"error\")\n",
    "                    #print(\"error\",words,\"/\",word)\n",
    "                    continue\n",
    "            sqe_num=len(text_sequence)\n",
    "            #print(\"line\",line,\"text_sequence\",sqe_num)\n",
    "            if(sqe_num==0):\n",
    "                break\n",
    "            if(sqe_num<sqe_limit):\n",
    "                #print(\"shape\",np.array(text_sequence).shape)\n",
    "                text_sequence=np.pad(np.array(text_sequence), ((0,sqe_limit-sqe_num),(0,0)), 'constant')\n",
    "                \n",
    "            samples.append(text_sequence)\n",
    "            \n",
    "        if label == \"good\":\n",
    "            labels = np.zeros(len(samples))\n",
    "            #labels = np.zeros(len(samples),dtype=\"int64\")\n",
    "            #labels = np.ones(len(samples))\n",
    "        elif label == \"bad\":\n",
    "            labels = np.ones(len(samples))\n",
    "            #labels = np.zeros(len(samples))-1\n",
    "            #labels = np.ones(len(samples),dtype=\"int64\")\n",
    "        if x == [] or y == [] :\n",
    "            x = samples\n",
    "            y = labels\n",
    "            \n",
    "        else:\n",
    "            x = np.append(x, samples, axis=0)\n",
    "            y = np.append(y, labels, axis=0)\n",
    "          \n",
    "        \n",
    "        f.close()\n",
    "    \n",
    "   \n",
    "      \n",
    "   \n",
    "            \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size,random_state=11)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "#def getTrainBatch(x_train,y_train, batch_num=0,batch_size=128,no_of_classes=2):\n",
    "def getTrainBatch(x_train,y_train, batch_num=0,batch_size=128):\n",
    "    #将data分成batch，并将字符转化为one-hot编码\n",
    "    data_size = len(x_train)\n",
    "    start_index = batch_num * batch_size\n",
    "    end_index = data_size if batch_size == 0 else min((batch_num + 1) *batch_size, data_size)\n",
    "     #获取第batch_num批数据（根据开始和结束索引）\n",
    "    batch_x = x_train[start_index:end_index]\n",
    "    batch_y = y_train[start_index:end_index]\n",
    "    #将单词转化为索引存储在batch_indices中\n",
    "    #batch_y=np.reshape(batch_y,[-1,1])\n",
    "    return batch_x,batch_y\n",
    "\n",
    "\n",
    "\n",
    "x_train,y_train,x_test,y_test=load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start...\n",
      "end...\n",
      "Loading data ....\n",
      "Loaded\n",
      "Training ===>\n",
      "  [TL] InputLayer  input_layer: (?, ?, 70)\n",
      "  [TL] BiDynamicRNNLayer dynamic_rnn1: n_hidden:64 in_dim:3 in_shape:(?, ?, 70) cell_fn:BasicLSTMCell dropout:0.8 n_layer:3\n",
      "       non specified batch_size, uses a tensor instead.\n",
      "     n_params : 12\n",
      "0\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:30.779948: step 1, loss 0.693203, acc 0.375\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:31.268109: step 2, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.5\n",
      "Saved model checkpoint to model/biModeltest\n",
      "\n",
      "1\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:35.614671: step 3, loss 0.693147, acc 0.5\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:36.124578: step 4, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "2\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:38.916540: step 5, loss 0.693131, acc 0.5\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:39.336012: step 6, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "3\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:41.874872: step 7, loss 0.69303, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:42.339144: step 8, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "4\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:45.150227: step 9, loss 0.693, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:45.640723: step 10, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "5\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:48.457610: step 11, loss 0.692915, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:48.978795: step 12, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "6\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:51.578712: step 13, loss 0.692802, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:52.108852: step 14, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "7\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:54.870479: step 15, loss 0.692685, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:55.357328: step 16, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "8\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:04:57.961469: step 17, loss 0.692485, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:04:58.433841: step 18, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "9\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:01.036067: step 19, loss 0.692321, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:01.545885: step 20, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "10\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:04.325280: step 21, loss 0.692142, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:04.787365: step 22, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "11\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:07.459291: step 23, loss 0.691839, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:07.959413: step 24, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "12\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:10.601921: step 25, loss 0.691488, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:11.081762: step 26, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "13\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:13.693240: step 27, loss 0.691191, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:14.203930: step 28, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "14\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:16.876152: step 29, loss 0.690768, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:17.356027: step 30, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "15\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:20.088066: step 31, loss 0.690417, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:20.568740: step 32, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "16\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:23.210439: step 33, loss 0.689974, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:23.700366: step 34, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "17\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:26.341722: step 35, loss 0.689502, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:26.838341: step 36, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "18\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:29.474176: step 37, loss 0.689096, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:30.004819: step 38, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "19\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:32.686651: step 39, loss 0.688632, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:33.184104: step 40, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "20\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:36.048524: step 41, loss 0.688217, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:36.359732: step 42, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "21\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:39.071119: step 43, loss 0.687711, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:39.561158: step 44, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "22\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:42.193084: step 45, loss 0.687274, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:42.663198: step 46, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "23\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:45.295087: step 47, loss 0.686831, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:45.815392: step 48, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "24\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:48.396865: step 49, loss 0.686374, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:48.887205: step 50, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "25\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:51.609512: step 51, loss 0.68589, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:52.109190: step 52, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "26\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:54.771778: step 53, loss 0.685419, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:55.355197: step 54, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "27\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:05:58.307093: step 55, loss 0.684983, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:05:58.833660: step 56, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "28\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:01.543584: step 57, loss 0.684513, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:01.929976: step 58, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "29\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:04.736683: step 59, loss 0.684054, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:05.246980: step 60, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "30\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:08.028741: step 61, loss 0.683567, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:08.518404: step 62, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "31\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:11.210144: step 63, loss 0.683035, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:11.739769: step 64, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "32\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:14.312236: step 65, loss 0.682607, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:14.791962: step 66, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "33\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:17.551382: step 67, loss 0.682044, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:18.023814: step 68, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "34\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:20.634208: step 69, loss 0.68153, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:21.114631: step 70, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "35\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:23.806726: step 71, loss 0.68103, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:24.226952: step 72, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "36\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:26.908159: step 73, loss 0.680517, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:27.397905: step 74, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:29.909730: step 75, loss 0.679874, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:30.380559: step 76, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "38\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:33.092535: step 77, loss 0.679281, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:33.563049: step 78, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "39\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:36.184428: step 79, loss 0.678799, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:36.662789: step 80, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "40\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:39.275668: step 81, loss 0.678191, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:39.796306: step 82, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "41\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:42.428047: step 83, loss 0.677698, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:42.908075: step 84, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "42\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:45.618917: step 85, loss 0.676898, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:46.099883: step 86, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "43\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:48.766643: step 87, loss 0.676343, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:49.241406: step 88, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "44\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:51.773080: step 89, loss 0.675572, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:52.269654: step 90, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "45\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:55.035628: step 91, loss 0.67487, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:55.545532: step 92, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "46\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:06:58.196483: step 93, loss 0.673646, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:06:58.677128: step 94, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "47\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:01.338909: step 95, loss 0.672066, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:01.819260: step 96, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "48\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:04.470330: step 97, loss 0.670293, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:04.941343: step 98, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "49\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:07.673690: step 99, loss 0.664312, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:08.224002: step 100, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "50\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:10.874991: step 101, loss 0.653753, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:11.355273: step 102, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "51\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:13.970798: step 103, loss 0.645138, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:14.448497: step 104, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "52\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:17.230680: step 105, loss 0.646905, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:17.711597: step 106, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "53\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:20.372801: step 107, loss 0.62772, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:20.853178: step 108, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "54\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:23.535897: step 109, loss 0.603918, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:24.066353: step 110, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "55\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:26.698144: step 111, loss 0.618695, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:27.178942: step 112, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "56\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:29.889683: step 113, loss 0.644808, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:30.369987: step 114, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "57\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:33.063138: step 115, loss 0.614457, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:33.550947: step 116, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "58\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:36.212643: step 117, loss 0.581141, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:36.693004: step 118, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "59\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:39.404713: step 119, loss 0.572899, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:39.894521: step 120, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "60\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:42.547253: step 121, loss 0.542936, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:43.047193: step 122, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "61\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:45.748683: step 123, loss 0.576699, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:46.229333: step 124, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "62\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:48.890554: step 125, loss 0.608483, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:49.408074: step 126, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "63\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:52.043127: step 127, loss 0.583193, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:52.533026: step 128, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "64\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:55.294680: step 129, loss 0.512, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:55.764658: step 130, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "65\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:07:58.435783: step 131, loss 0.482034, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:07:58.855778: step 132, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "66\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:01.567060: step 133, loss 0.492149, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:02.057506: step 134, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "67\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:04.740098: step 135, loss 0.50752, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:05.230088: step 136, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "68\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:07.901006: step 137, loss 0.430609, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:08.321686: step 138, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "69\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:10.973871: step 139, loss 0.489468, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:11.444820: step 140, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "70\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:14.126536: step 141, loss 0.44846, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:14.606740: step 142, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "71\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:17.338637: step 143, loss 0.428526, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:17.825356: step 144, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "72\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:20.630381: step 145, loss 0.433573, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:21.121346: step 146, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "73\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:23.702260: step 147, loss 0.428138, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:24.181826: step 148, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "74\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:26.923864: step 149, loss 0.373501, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:27.403357: step 150, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "75\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:30.085918: step 151, loss 0.607084, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-07T17:08:30.595628: step 152, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.5\n",
      "76\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:33.307234: step 153, loss 0.550391, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:33.786941: step 154, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "77\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:36.439246: step 155, loss 0.296075, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:36.908782: step 156, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "78\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:39.690748: step 157, loss 0.369559, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:40.180337: step 158, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "79\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:42.902841: step 159, loss 0.295791, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:43.393092: step 160, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "80\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:45.978551: step 161, loss 0.160509, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:46.454695: step 162, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "81\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:49.295957: step 163, loss 0.216201, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:49.766613: step 164, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.5\n",
      "82\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:52.468253: step 165, loss 0.19422, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:52.947942: step 166, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.5\n",
      "83\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:55.659675: step 167, loss 0.247183, acc 0.75\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:56.160094: step 168, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "84\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:08:58.850006: step 169, loss 0.145212, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:08:59.323498: step 170, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "85\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:02.125383: step 171, loss 0.237676, acc 0.75\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:02.605612: step 172, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 1.0\n",
      "Saved model checkpoint to model/biModeltest\n",
      "\n",
      "86\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:07.018382: step 173, loss 1.2624, acc 0.625\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:07.508814: step 174, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "87\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:10.231331: step 175, loss 0.328819, acc 0.75\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:10.661427: step 176, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "88\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:13.342905: step 177, loss 0.544394, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:13.823725: step 178, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "89\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:16.625334: step 179, loss 0.218466, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:17.135965: step 180, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "90\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:19.867142: step 181, loss 0.223668, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:20.287860: step 182, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "91\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:22.929807: step 183, loss 0.417214, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:23.439716: step 184, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 1.0\n",
      "92\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:26.223557: step 185, loss 0.170878, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:26.711914: step 186, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.5\n",
      "93\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:29.473934: step 187, loss 0.101641, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:29.944484: step 188, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "94\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:32.656430: step 189, loss 0.0933297, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:33.132096: step 190, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.5\n",
      "95\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:35.848119: step 191, loss 0.0854096, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:36.336982: step 192, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "96\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:39.160313: step 193, loss 0.0864132, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:39.460079: step 194, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "97\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:42.162272: step 195, loss 0.0497683, acc 1\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:42.682035: step 196, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n",
      "98\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:45.384196: step 197, loss 0.150508, acc 0.875\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:45.854431: step 198, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 1.0\n",
      "99\n",
      "x_batch (8, 256, 70) y_batch (8,)\n",
      "2018-04-07T17:09:48.565210: step 199, loss 1.02406, acc 0.75\n",
      "0 / 2\n",
      "x_batch (0, 256, 70) y_batch (0,)\n",
      "2018-04-07T17:09:48.945083: step 200, loss nan, acc nan\n",
      "1 / 2\n",
      "test_acc 0.0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "#import config\n",
    "#from data_utils import Data\n",
    "#from char_cnn import CharConvNet\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "alphabet=\" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "alphabet_size = len(alphabet)\n",
    "\n",
    "#batch_size=128\n",
    "batch_size=5\n",
    "\n",
    "#no_of_classes = 2\n",
    "#--\n",
    "\n",
    "\n",
    "#--\n",
    "dropout_keep_prob=0.9\n",
    "base_rate = 1e-2\n",
    "#momentum = 0.9\n",
    "\n",
    "\n",
    "epoches = 100\n",
    "#evaluate_every = 50\n",
    "#checkpoint_every = 100\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print ('start...')\n",
    "    #execfile(\"config.py\")\n",
    "    #print config.model.th\n",
    "    print ('end...')\n",
    "    print (\"Loading data ....\")\n",
    " \n",
    "    \n",
    "    \n",
    "\n",
    "    num_batches_per_epoch = int(len(x_train) / batch_size) + 1\n",
    "    num_batch_dev = len(x_test)\n",
    "    print (\"Loaded\")\n",
    " \n",
    "    print (\"Training ===>\")\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement = True,\n",
    "                                      log_device_placement = False)\n",
    "\n",
    "        sess = tf.Session(config = session_conf)\n",
    "\n",
    "        with sess.as_default():\n",
    "            network = Network()\n",
    "\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            optimizer=tf.train.RMSPropOptimizer(learning_rate,decay=0.8,centered=True,use_locking=True)\n",
    "            #(learning_rate, decay=0.9, momentum=0.0, epsilon=1e-10, use_locking=False, centered=False, name='RMSProp')\n",
    "            grads_and_vars = optimizer.compute_gradients(network.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step)\n",
    "            \n",
    "  \n",
    "\n",
    "           \n",
    "           \n",
    "           \n",
    "           \n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            checkpoint_prefix = \"model/biModel\"+dataset_name                          \n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                print(\"x_batch\",x_batch.shape,\"y_batch\",y_batch.shape)\n",
    "                feed_dict = {\n",
    "                  network.x: x_batch,\n",
    "                  network.y: y_batch,\n",
    "                  network.dropout_keep_prob: dropout_keep_prob\n",
    "                }\n",
    "                _, step, loss, accuracy = sess.run(\n",
    "                    [train_op,\n",
    "                     global_step,\n",
    "                     network.loss,\n",
    "                     network.accuracy],\n",
    "                    feed_dict)\n",
    "                \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                #print(loss)\n",
    "                #newloss=np.mean(loss)\n",
    "                #print(newloss)\n",
    "                #print(accuracy)\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    " \n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  network.x: x_batch,\n",
    "                  network.y: y_batch,\n",
    "                  network.dropout_keep_prob: 1.0 # Disable dropout\n",
    "                }\n",
    "                step,loss, accuracy = sess.run(\n",
    "                    [global_step,\n",
    "                    \n",
    "                     network.loss,\n",
    "                     network.accuracy],\n",
    "                    feed_dict)\n",
    "                \n",
    "                print(\"{}: step {}, loss {:g}, test_acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            \"\"\"  \n",
    "            def getPadding(batch_x):\n",
    "                max_seq_len = max([len(d) for d in batch_x])\n",
    "                for i,d in enumerate(batch_x):\n",
    "                    batch_x[i] =batch_x[i]+ [np.zeros(alphabet_size) for num in range(max_seq_len - len(d))]\n",
    "                batch_x = list(batch_x) # ValueError: setting an array element with a sequence.\n",
    "                    #batch_x=batch_x[:l_0]\n",
    "                return batch_x\n",
    "            \"\"\"  \n",
    "            best_acc=0\n",
    "            for e in range(epoches):\n",
    "                print (e)\n",
    "                #train_data.shuffleData()\n",
    "                for k in range(num_batches_per_epoch):\n",
    "\n",
    "                    batch_x, batch_y = getTrainBatch(x_train,y_train,k)\n",
    "                    \n",
    "                    ##padding\n",
    "                    \"\"\"\n",
    "                    max_seq_len = max([len(d) for d in batch_x])\n",
    "                    for i,d in enumerate(batch_x):\n",
    "                        batch_x[i] =batch_x[i]+ [np.zeros(alphabet_size) for num in range(max_seq_len - len(d))]\n",
    "                    #batch_x = list(batch_x) # ValueError: setting an array element with a sequence.\n",
    "                    #batch_x=batch_x[:l_0]\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    ##train\n",
    "                    train_step(batch_x, batch_y)\n",
    "                    print(k,\"/\",num_batches_per_epoch)\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    \n",
    "                        \n",
    "                #test acc\n",
    "                y_test=np.reshape(y_test,[-1,])\n",
    "                feed_dict = {network.x: x_test, network.y: y_test}\n",
    "                #correct   = tf.equal(network.network.outputs_op, network.y)\n",
    "                #accuracy  = tf.reduce_mean(tf.cast(correct,tf.float32),axis=0)\n",
    "                \n",
    "                accuracy  = tf.reduce_mean(tf.cast(tf.nn.in_top_k(network.logits, network.y, 1), tf.float32))\n",
    "                test_acc  = sess.run(accuracy, feed_dict=feed_dict)\n",
    "\n",
    "                print( \"test_acc\",test_acc) #softmax acc= 0.8965\n",
    "                if(test_acc>best_acc):\n",
    "                    best_acc=test_acc\n",
    "                    path = saver.save(sess, checkpoint_prefix)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))        \n",
    "                path = saver.save(sess, checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, y_train, x_test, y_test = load_dataset()\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "y_test=np.reshape(y_test,[-1,1])\n",
    "feed_dict = {network.x: x_test, network.y: y_test}\n",
    "correct   = tf.equal(network.network.outputs_op, tf.cast(network.y,tf.int64))\n",
    "accuracy  = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "#test_acc  = sess.run(accuracy, feed_dict=feed_dict)\n",
    "#print(\"accurancy\",test_acc)   \n",
    "print( sess.run(accuracy, feed_dict=feed_dict)) #softmax acc= 0.8965    sigoid acc=0.903167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 17, 70)\n",
      "  [TL] InputLayer  input_layer: (?, ?, 70)\n",
      "  [TL] BiDynamicRNNLayer dynamic_rnn1: n_hidden:64 in_dim:3 in_shape:(?, ?, 70) cell_fn:BasicLSTMCell dropout:1.0 n_layer:3\n",
      "       non specified batch_size, uses a tensor instead.\n",
      "     n_params : 12\n",
      "INFO:tensorflow:Restoring parameters from model/biModeltest\n",
      "offensive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x205e0f5c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAC6CAYAAAAJZ4xBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nck6BbUEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x205e0fc3898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def draw_attention_map(input_text,attention_weights):\n",
    "    output_length=1\n",
    "\n",
    "\n",
    "    attention_weights=attention_weights.reshape((1,attention_weights.shape[1]))\n",
    "    predicted_text=\" \"\n",
    "    input_length=len(input_text)\n",
    "    text_=list(input_text)\n",
    "    #Plot the attention_map\n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 3.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "    attention_map = np.zeros((1, input_length))\n",
    "    \n",
    "    Ty, Tx = attention_map.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for t_prime in range(Tx):\n",
    "        attention_map[0][t_prime] = attention_weights[0][t_prime]\n",
    "\n",
    "\n",
    "    # add image\n",
    "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=0)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    #x.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "    f.show()\n",
    "#tf.reset_default_graph()\n",
    "def getSquencetCharvect(line):\n",
    "    \n",
    "    \n",
    "    words = list(line.strip())\n",
    "    text_sequence = []\n",
    "    for word in words:\n",
    "        text_sequence.append(one_hot_list[alpha2id_dict[word]])\n",
    "    #print(text_sequence)    \n",
    "    return text_sequence\n",
    "#input_x=np.array(getSquencetCharvect(\"they probably in he!l\"))#\n",
    "#input_x=np.array(getSquencetCharvect(\"pkk claims terrorist attack in revenge for army shelling in northern\"))#\n",
    "#tf.reset_default_graph()\n",
    "#input_x=np.array(getSquencetCharvect(\"love\"))\n",
    "#input_x=np.array(getSquencetCharvect(\"f*cking\"))\n",
    "#input_sent=\"dude i said i was sorry what you dont give a fuck\"\n",
    "input_sent=\"you are a bad man\"\n",
    "input_x=np.array(getSquencetCharvect(input_sent))\n",
    "input_x=np.expand_dims(input_x,0)\n",
    "print(input_x.shape)\n",
    "\n",
    "censor_sess=tf.Session()\n",
    "network = Network( dropout_keep_prob=1.0)  \n",
    "tf.train.Saver().restore(censor_sess,\"model/biModeltest\")\n",
    "\n",
    "feed_dict = {network.x: input_x}\n",
    "result=censor_sess.run(network.network.outputs_op,feed_dict)\n",
    "\n",
    "attention_weights=censor_sess.run(network.attention_weights,feed_dict)\n",
    "#print(\"attention_weights\",attention_weights,np.array(attention_weights).shape)\n",
    "draw_attention_map(input_sent,attention_weights)\n",
    "#attention\n",
    "if(result==[0]):\n",
    "    print(\"normal\")\n",
    "else:\n",
    "    print(\"offensive\")\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights [[[ 0.84622663]\n",
      "  [ 0.09629885]\n",
      "  [ 0.01464968]\n",
      "  [ 0.00401267]\n",
      "  [ 0.00209272]\n",
      "  [ 0.00182676]\n",
      "  [ 0.00241827]\n",
      "  [ 0.00435293]\n",
      "  [ 0.00913139]\n",
      "  [ 0.01899021]]] (1, 10, 1)\n",
      "normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2040ec10828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAADDCAYAAAAlStBIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRFoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x205e128d9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "input_sent=\"i love you\"\n",
    "input_x=np.array(getSquencetCharvect(input_sent))\n",
    "input_x=np.expand_dims(input_x,0)\n",
    "\n",
    "feed_dict = {network.x: input_x}\n",
    "result=censor_sess.run(network.network.outputs_op,feed_dict)\n",
    "\n",
    "attention_weights=censor_sess.run(network.attention_weights,feed_dict)\n",
    "print(\"attention_weights\",attention_weights,np.array(attention_weights).shape)\n",
    "draw_attention_map(input_sent,attention_weights)\n",
    "#attention\n",
    "if(result==[0]):\n",
    "    print(\"normal\")\n",
    "else:\n",
    "    print(\"offensive\")\n",
    "#censor_sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## profane words detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] InputLayer  input_layer: (?, ?, 70)\n",
      "  [TL] DynamicRNNLayer dynamic_rnn1: n_hidden:64, in_dim:3 in_shape:(?, ?, 70) cell_fn:BasicLSTMCell dropout:1.0 n_layer:3\n",
      "       non specified batch_size, uses a tensor instead.\n",
      "  [TL] DenseLayer  output: 1 identity\n",
      "INFO:tensorflow:Restoring parameters from model/biModelWord\n",
      "a$$ is offensive\n",
      "@ss is offensive\n",
      "sh!t is offensive\n",
      "$hit is offensive\n",
      "f*** is offensive\n",
      "f*cking is offensive\n"
     ]
    }
   ],
   "source": [
    "def getSquencetCharvect(line):\n",
    "    words = list(line.strip())\n",
    "    text_sequence = []\n",
    "    for word in words:\n",
    "        text_sequence.append(one_hot_list[alpha2id_dict[word]]) \n",
    "    return text_sequence\n",
    "\n",
    "\n",
    "wordsList=['a$$','@ss','sh!t','$hit','f***','f*cking']\n",
    "with tf.Session() as censor_sess:\n",
    "    network = Network( dropout_keep_prob=1.0)  \n",
    "    tf.train.Saver().restore(censor_sess,\"model/biModelWord\")\n",
    "    for word in wordsList:\n",
    "        input_x=np.array(getSquencetCharvect(word))\n",
    "        input_x=np.expand_dims(input_x,0)\n",
    "\n",
    "        feed_dict = {network.x: input_x}\n",
    "        result=censor_sess.run(network.network.outputs_op,feed_dict)\n",
    "        if(result==[0]):\n",
    "            print(word+\" is normal\")\n",
    "        else:\n",
    "            print(word+\" is offensive\")\n",
    "        #print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "loss=[[1,2,3]]\n",
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
